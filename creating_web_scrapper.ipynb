{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as Bs \n",
    "import re \n",
    "import lxml\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html(url):\n",
    "    time.sleep(1)\n",
    "    res = requests.get(url)\n",
    "    res.raise_for_status() \n",
    "    return res.content\n",
    "\n",
    "def get_soup(html):\n",
    "    return Bs(html, \"html.parser\")\n",
    "\n",
    "def get_href(soup):\n",
    "    return [i if i.startswith(\"/\") else \"/\" + i for i in [i[\"href\"] for i in soup.find_all(\"a\")[:-1]]]\n",
    "\n",
    "def generate_url(href):\n",
    "    return [f\"https://planecrashinfo.com{i}\" for i in href]\n",
    "\n",
    "# int(test_soup.find(\"title\").text)\n",
    "def generate_url_outside(href):\n",
    "    return [f\"https://planecrashinfo.com/{int(i.split('-')[0][1:])}{i}\" for i in href]\n",
    "\n",
    "def decompress_list(lis):\n",
    "    return [m for i in lis for m in i]\n",
    "\n",
    "def extract_data(soup_table):\n",
    "    accident_data = {}\n",
    "    # Loop through rows and extract key-value pairs\n",
    "    for row in soup_table[1:]:  # Skip header rows\n",
    "        key_cell = row.find('td', align='right')\n",
    "        value_cell = row.find('td', align='left')\n",
    "        if key_cell and value_cell:\n",
    "            key = key_cell.text.strip().replace(':', '')  # Remove colon and trailing spaces\n",
    "            value = value_cell.text.strip()\n",
    "            accident_data[key] = value\n",
    "    return accident_data\n",
    "\n",
    "def first_page_pass(url):\n",
    "    html = get_html(url)\n",
    "    soup = get_soup(html)\n",
    "    href = get_href(soup)\n",
    "    href_url = generate_url(href)\n",
    "    return href_url\n",
    "\n",
    "def second_page_pass(href_url):\n",
    "    return [generate_url_outside(i) for i in [get_href(i) for i in [get_soup(i) for i in [get_html(i) for i in href_url]]]]\n",
    "\n",
    "def third_final_pass(good_url):\n",
    "    return [extract_data(i) for i in [get_soup(j).find_all(\"tr\") for j in [get_html(i) for i in good_url ]]]\n",
    "\n",
    "def save_csv(my_list,name):\n",
    "    j = pd.DataFrame(my_list)\n",
    "    j.to_csv(f\"data/{name}\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.planecrashinfo.com/database.htm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "href_url = first_page_pass(url)\n",
    "good_url = second_page_pass(href_url)\n",
    "save_csv(good_url,\"urls/\"+\"list_of_list_urls.csv\")\n",
    "k = decompress_list(good_url)\n",
    "save_csv(k,\"urls/\"+\"list_of_urls.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105, 5037)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(good_url),len(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "403 Client Error: Forbidden for url: https://planecrashinfo.com/2005/2005-5.htm",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m good_url:\n\u001b[1;32m      2\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m     new_list \u001b[38;5;241m=\u001b[39m \u001b[43mthird_final_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     save_csv(new_list,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiles/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m i[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 46\u001b[0m, in \u001b[0;36mthird_final_pass\u001b[0;34m(good_url)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mthird_final_pass\u001b[39m(good_url):\n\u001b[0;32m---> 46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [extract_data(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m [get_soup(j)\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m [\u001b[43mget_html\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m good_url ]]]\n",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m, in \u001b[0;36mget_html\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m      2\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      3\u001b[0m res \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mres\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[0;32m~/miniforge3/envs/project_2/lib/python3.12/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1016\u001b[0m     http_error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1017\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1018\u001b[0m     )\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 403 Client Error: Forbidden for url: https://planecrashinfo.com/2005/2005-5.htm"
     ]
    }
   ],
   "source": [
    "for i in good_url:\n",
    "    time.sleep(3)\n",
    "    new_list = third_final_pass(i)\n",
    "    save_csv(new_list,\"files/\" + i[0].split(\"/\")[-2] + \".csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
