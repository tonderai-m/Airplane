{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as Bs \n",
    "import re \n",
    "import lxml\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html(url):\n",
    "    time.sleep(1)\n",
    "    res = requests.get(url)\n",
    "    res.raise_for_status() \n",
    "    return res.content\n",
    "\n",
    "def get_soup(html):\n",
    "    return Bs(html, \"html.parser\")\n",
    "\n",
    "def get_href(soup):\n",
    "    return [i if i.startswith(\"/\") else \"/\" + i for i in [i[\"href\"] for i in soup.find_all(\"a\")[:-1]]]\n",
    "\n",
    "def generate_url(href):\n",
    "    return [f\"https://planecrashinfo.com{i}\" for i in href]\n",
    "\n",
    "# int(test_soup.find(\"title\").text)\n",
    "def generate_url_outside(href):\n",
    "    return [f\"https://planecrashinfo.com/{int(i.split('-')[0][1:])}{i}\" for i in href]\n",
    "\n",
    "def decompress_list(lis):\n",
    "    return [m for i in lis for m in i]\n",
    "\n",
    "def extract_data(soup_table):\n",
    "    accident_data = {}\n",
    "    # Loop through rows and extract key-value pairs\n",
    "    for row in soup_table[2:]:  # Skip header rows\n",
    "        key_cell = row.find('td', align='right')\n",
    "        value_cell = row.find('td', align='left')\n",
    "        if key_cell and value_cell:\n",
    "            key = key_cell.text.strip().replace(':', '')  # Remove colon and trailing spaces\n",
    "            value = value_cell.text.strip()\n",
    "            accident_data[key] = value\n",
    "    return accident_data\n",
    "\n",
    "def first_page_pass(url):\n",
    "    html = get_html(url)\n",
    "    soup = get_soup(html)\n",
    "    href = get_href(soup)\n",
    "    href_url = generate_url(href)\n",
    "    return href_url\n",
    "\n",
    "def second_page_pass(href_url):\n",
    "    return [generate_url_outside(i) for i in [get_href(i) for i in [get_soup(i) for i in [get_html(i) for i in href_url]]]]\n",
    "\n",
    "def third_final_pass(good_url):\n",
    "    return [extract_data(i) for i in [get_soup(j).find_all(\"tr\") for j in [get_html(i) for i in good_url ]]]\n",
    "\n",
    "def save_csv(my_list):\n",
    "    j = pd.DataFrame(my_list)\n",
    "    j.to_csv(\"data/list_of_urls.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://planecrashinfo.com/database.htm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "href_url = first_page_pass(url)\n",
    "good_url = second_page_pass(href_url)\n",
    "k = decompress_list(good_url)\n",
    "save_csv(k)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
